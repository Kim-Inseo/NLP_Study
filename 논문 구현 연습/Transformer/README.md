- 아무 것도 없는 상태에서 직접 구현한 것은 아니며 Reference를 참고하면서 이해하며 작성함.

## 작성 기간
2024.10.20. ~ 2024.10.24.

## Reference
- https://arxiv.org/pdf/1706.03762 (Attention Is All You Need, 논문)

- https://github.com/daehwichoi/transformer-pytorch/blob/main/model/transformer.py (코드)

- https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb (코드)

- https://velog.io/@heiswicked/%ED%86%A0%EC%B9%98%EC%9D%98-%ED%98%B8%ED%9D%A1-06-About-Transformer-PART-01-PositionalEncodingLayer (이론 & 코드)

- https://yangoos57.github.io/blog/DeepLearning/paper/Transformer/Transformer_From_Scratch (이론 & 코드)
    - https://github.com/yangoos57/Transformer_from_scratch/blob/main/%5Btutorial%5D%20training%08transformer%20.ipynb (코드)

- https://cpm0722.github.io/pytorch-implementation/transformer (이론 & 코드)
    - https://github.com/cpm0722/transformer_pytorch/blob/main/models/model/transformer.py (코드)

- https://github.com/loggerJK/transformer-implementation/blob/master/Training.ipynb (코드)
